{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Validating: Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "||continuous|categorical|\n",
    "|---|---|---|\n",
    "|**supervised**|**regression**|**classification**|\n",
    "|unsupervised|dimension reduction|clustering|\n",
    "\n",
    "In the previous notebook we saw, Random Forest at work. In this one we are taking a look at how well its working...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision versus Recall\n",
    "\n",
    "* **Precision** (also called positive predictive value) is the fraction of relevant instances among the retrieved instances\n",
    "* **Recall** (also known as sensitivity) is the fraction of relevant instances that have been retrieved over the total amount of relevant instances. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Both precision and recall are therefore based on an understanding and measure of relevance.\n",
    "\n",
    "***For Example*** : \n",
    "- Suppose a computer program for recognizing dogs in photographs identifies eight dogs in a picture containing 12 dogs and some cats. \n",
    "    - Of the eight dogs identified, five actually are dogs (true positives), while the rest are cats (false positives). \n",
    "    - The program's precision is 5/8 while its recall is 5/12. \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "- When a search engine returns 30 pages, only 20 of which were relevant while failing to return 40 additional relevant pages. \n",
    "    - its precision is 20/30 = 2/3 while its recall is 20/60 = 1/3. \n",
    "    - So, in this case, precision is \"how useful the search results are\", and recall is \"how complete the results are\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div id=\"container\"> <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/440px-Precisionrecall.svg.png\" class=\"center\" alt=\"Random Forest\" style=\"width: 300px;\"/> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In statistics, if the null hypothesis is that - ***all items are irrelevant***\n",
    "- where the hypothesis is accepted or rejected based on the number selected compared with the sample size \n",
    "- absence of type I and type II errors corresponds respectively to maximum precision (no false positive) and maximum recall (no false negative). \n",
    "- The above pattern recognition example\n",
    "    - 8 dogs in a picture containing 12 dogs and some cats, \n",
    "        - 5 actually are dogs (true positives), while the rest are cats (false positives)\n",
    "    - contained 8 − 5 = 3 **Type I errors**, and 12 − 5 = 7 **Type II errors.** \n",
    "    \n",
    "    \n",
    "- **Precision** can be seen as a measure of ***exactness or quality***, whereas **Recall** is a measure of ***completeness or quantity.*** \n",
    "\n",
    "In simple terms, high precision means that an algorithm returned substantially more relevant results than irrelevant ones, while high recall means that an algorithm returned most of the relevant results. [Source](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**$F1\\_Score$** (also F-score or F-measure) is a measure of a test's accuracy\n",
    "\n",
    "$$F1\\_Score = 2*\\frac{precison * recall}{precision + recall}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Another Example: Lets complicate the model a little more\n",
    "\n",
    "Reload Digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Learn to predict each class against the other\n",
    "from sklearn import svm\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Binarize the labels and use SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Binarize the output\n",
    "y = label_binarize(y, classes=range(0,10))\n",
    "n_classes = y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Do a train-test split and train the classifier -  Here we are trying to compare the predictive power of each class to other classes so [OneVsRestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.7,\n",
    "                                                    random_state=0)\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n",
    "                                 random_state=random_state))\n",
    "fit_model = classifier.fit(X_train, y_train)\n",
    "y_score = fit_model.decision_function(X_test)\n",
    "y_pred_svm = fit_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Accuracy report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97       130\n",
      "           1       0.90      0.86      0.88       129\n",
      "           2       1.00      0.98      0.99       122\n",
      "           3       0.91      0.80      0.85       148\n",
      "           4       0.94      0.97      0.95       117\n",
      "           5       0.91      1.00      0.95       127\n",
      "           6       0.94      0.98      0.96       122\n",
      "           7       0.97      0.93      0.95       121\n",
      "           8       0.77      0.70      0.73       140\n",
      "           9       0.89      0.95      0.92       117\n",
      "\n",
      "   micro avg       0.92      0.91      0.91      1273\n",
      "   macro avg       0.92      0.91      0.92      1273\n",
      "weighted avg       0.92      0.91      0.91      1273\n",
      " samples avg       0.92      0.88      0.89      1273\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jarvis/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_pred_svm, y_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>Reference [GitHub](https://github.com/jakevdp/sklearn_pycon2015/).</i></small>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
